"""
app/services/flexible_csv_service.py

å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆï¼ˆWBGTå®Ÿè£…å«ã‚€ï¼‰
"""

import pandas as pd
import io
from fastapi import UploadFile, HTTPException
from sqlalchemy.orm import Session
from typing import Optional, Dict, Any, List
from datetime import datetime
from app.models.flexible_sensor_data import (
    RawSensorData, FlexibleSensorMapping,
    SkinTemperatureData, CoreTemperatureData, 
    HeartRateData, WBGTData, SensorDataStatus, SensorType
)
from app.schemas.sensor_data import (
    UploadResponse, MappingResponse,
    DataSummaryResponse, MappingStatusResponse
)

class FlexibleCSVService:
    
    async def process_sensor_data_only(
        self,
        sensor_file: UploadFile,
        sensor_type: SensorType,
        competition_id: str,
        db: Session
    ) -> UploadResponse:
        """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿å‡¦ç†"""
        try:
            # å¤§ä¼šå­˜åœ¨ãƒã‚§ãƒƒã‚¯è¿½åŠ 
            from app.models import Competition
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            content = await sensor_file.read()
            df = pd.read_csv(io.BytesIO(content))
            
            processed = 0
            
            for _, row in df.iterrows():
                raw_data = RawSensorData(
                    sensor_type=sensor_type,
                    sensor_id=str(row.get('sensor_id', '')),
                    timestamp=pd.to_datetime(row.get('timestamp')),
                    data_values=str(row.get('value', 0)),
                    competition_id=competition_id,
                    mapping_status=SensorDataStatus.UNMAPPED,
                    raw_data=row.to_json()
                )
                db.add(raw_data)
                processed += 1
            
            db.commit()
            
            return UploadResponse(
                success=True,
                message=f"{sensor_type.value}ãƒ‡ãƒ¼ã‚¿ã‚’{processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸï¼ˆå¤§ä¼š: {competition.name}ï¼‰",
                total_records=len(df),
                processed_records=processed
            )
            
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=400, detail=f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    async def process_wbgt_data(
        self,
        wbgt_file: UploadFile,
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> UploadResponse:
        """WBGTç’°å¢ƒãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆè¨ˆæ¸¬æ™‚åˆ»ã‚’timestampã«ä¿å­˜ï¼‰"""
        try:
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp_str}_{wbgt_file.filename}"
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
            if overwrite:
                deleted_count = db.query(WBGTData).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜WBGTãƒ‡ãƒ¼ã‚¿{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # CSVèª­ã¿è¾¼ã¿
            content = await wbgt_file.read()
            decoded_content = content.decode('shift_jis')
            df = pd.read_csv(io.StringIO(decoded_content))
            
            # åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°
            column_mapping = {
                'date': 'æ—¥ä»˜',
                'time': 'æ™‚åˆ»',
                'wbgt': 'WBGTå€¤',
                'air_temperature': 'æ°—æ¸©',
                'humidity': 'ç›¸å¯¾æ¹¿åº¦',
                'globe_temperature': 'é»’çƒæ¸©åº¦'
            }
            
            processed = 0
            errors = []
            
            for idx, row in df.iterrows():
                try:
                    # è¨ˆæ¸¬æ™‚åˆ»ã‚’ timestamp ã«
                    date_str = str(row[column_mapping['date']]).strip()
                    time_str = str(row[column_mapping['time']]).strip()
                    dt = datetime.strptime(f"{date_str} {time_str}", '%Y/%m/%d %H:%M:%S')
                    
                    wbgt_value = float(row[column_mapping['wbgt']])
                    air_temp = float(row[column_mapping['air_temperature']])
                    humidity = float(row[column_mapping['humidity']])
                    globe_temp = float(row[column_mapping['globe_temperature']])
                    
                    wbgt_data = WBGTData(
                        timestamp=dt,
                        wbgt_value=wbgt_value,
                        air_temperature=air_temp,
                        humidity=humidity,
                        globe_temperature=globe_temp,
                        competition_id=competition_id,
                        upload_batch_id=batch_id
                    )
                    db.add(wbgt_data)
                    processed += 1
                except Exception as e:
                    errors.append(f"è¡Œ{idx+1}: {e}")
                    continue
            
            # UploadBatchç™»éŒ²
            from app.models.flexible_sensor_data import UploadBatch, UploadStatus, SensorType
            batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.WBGT,
                competition_id=competition_id,
                file_name=wbgt_file.filename,
                file_size=len(content),
                total_records=len(df),
                success_records=processed,
                failed_records=len(errors),
                status=UploadStatus.SUCCESS if not errors else UploadStatus.PARTIAL,
                uploaded_by="admin",
                notes=f"ã‚¨ãƒ©ãƒ¼{len(errors)}ä»¶" if errors else None
            )
            db.add(batch)
            db.commit()
            
            return UploadResponse(
                success=True,
                message=f"WBGTãƒ‡ãƒ¼ã‚¿ {processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸ",
                total_records=len(df),
                processed_records=processed,
                errors=errors[:20]  # æœ€å¤§20ä»¶
            )
            
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=400, detail=f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _normalize_wbgt_columns(self, columns: List[str]) -> Dict[str, str]:
        """WBGTåˆ—åã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        columns = [str(col).strip() for col in columns]
        mapping = {}
        
        # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªåˆ—åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        column_map = {
            'æ—¥ä»˜': 'date',
            'æ™‚åˆ»': 'time', 
            'WBGTå€¤': 'wbgt',
            'æ°—æ¸©': 'air_temperature',
            'ç›¸å¯¾æ¹¿åº¦': 'humidity',
            'é»’çƒæ¸©åº¦': 'globe_temperature'
        }
        
        # æ­£ç¢ºãªåˆ—åãƒãƒƒãƒãƒ³ã‚°
        for col in columns:
            if col in column_map:
                mapping[column_map[col]] = col
        
        # æœ€ä½é™WBGTå€¤ãŒå¿…è¦
        if 'wbgt' not in mapping:
            return {}
        
        return mapping

    def _combine_date_time(self, row: pd.Series, column_mapping: Dict[str, str]) -> Optional[datetime]:
        """æ—¥ä»˜ã¨æ™‚åˆ»ã‚’çµåˆã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        try:
            date_col = column_mapping.get('date')  # 'æ—¥ä»˜'
            time_col = column_mapping.get('time')  # 'æ™‚åˆ»'
            
            if not date_col or not time_col:
                return None
                
            date_str = str(row.get(date_col, '')).strip()  # '2025/07/15'
            time_str = str(row.get(time_col, '')).strip()  # '17:43:38'
            
            if not date_str or not time_str or date_str == 'nan' or time_str == 'nan':
                return None
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: '2025/07/15 17:43:38'
            datetime_str = f"{date_str} {time_str}"
            
            try:
                # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                return datetime.strptime(datetime_str, '%Y/%m/%d %H:%M:%S')
            except ValueError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    return pd.to_datetime(datetime_str)
                except:
                    return None
            
        except Exception as e:
            print(f"æ—¥ä»˜ãƒ»æ™‚åˆ»çµåˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨ã«floatå¤‰æ›"""
        if value is None or pd.isna(value):
            return None
        
        try:
            return float(value)
        except (ValueError, TypeError):
            return None

    async def process_mapping_data(
        self,
        mapping_file: UploadFile,
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> Dict[str, Any]:
        """ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆæŸ”è»Ÿãªåˆ—æ§‹æˆå¯¾å¿œï¼‰"""
        df = None  # å¤‰æ•°ã‚’æœ€åˆã«åˆæœŸåŒ–
        
        try:
            # å¤§ä¼šå­˜åœ¨ãƒã‚§ãƒƒã‚¯
            from app.models.competition import Competition
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            if overwrite and competition_id:
                deleted_count = db.query(FlexibleSensorMapping).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                await mapping_file.seek(0)
                content = await mapping_file.read()
            except Exception:
                # seekå¤±æ•—æ™‚ã¯æ—¢å­˜contentã‚’ä½¿ç”¨
                content = await mapping_file.read()
            
            if not content:
                raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
            decoded_content = None
            detected_encoding = None
            
            for encoding in ['utf-8', 'shift_jis', 'cp932', 'iso-8859-1']:
                try:
                    decoded_content = content.decode(encoding)
                    detected_encoding = encoding
                    break
                except UnicodeDecodeError:
                    continue
            
            if decoded_content is None:
                raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
            
            # CSVã‚’DataFrameã«èª­ã¿è¾¼ã¿
            try:
                df = pd.read_csv(io.StringIO(decoded_content))
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if df is None or df.empty:
                raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº† - è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
            print(f"åˆ—å: {list(df.columns)}")
            
            # åˆ—åã®ç©ºç™½é™¤å»
            df.columns = df.columns.str.strip()
            
            # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
            if 'user_id' not in df.columns:
                raise HTTPException(status_code=400, detail="user_idåˆ—ãŒå¿…è¦ã§ã™")
            
            # user_idé‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicate_users = df[df['user_id'].duplicated()]['user_id'].tolist()
            if duplicate_users:
                raise HTTPException(
                    status_code=400, 
                    detail=f"user_idã«é‡è¤‡ãŒã‚ã‚Šã¾ã™: {duplicate_users}"
                )
            
            # èªè­˜ã™ã‚‹ã‚»ãƒ³ã‚µãƒ¼åˆ—ï¼ˆæŸ”è»Ÿå¯¾å¿œï¼‰
            recognized_sensor_columns = {
                'skin_temp_sensor_id': SensorType.SKIN_TEMPERATURE,
                'core_temp_sensor_id': SensorType.CORE_TEMPERATURE,
                'heart_rate_sensor_id': SensorType.HEART_RATE,
                'skin_temperature_sensor_id': SensorType.SKIN_TEMPERATURE,  # åˆ¥åå¯¾å¿œ
                'core_temperature_sensor_id': SensorType.CORE_TEMPERATURE,  # åˆ¥åå¯¾å¿œ
                'heart_rate_id': SensorType.HEART_RATE,  # åˆ¥åå¯¾å¿œ
            }
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            print(f"èªè­˜å¯èƒ½ãªåˆ—: {list(recognized_sensor_columns.keys())}")
            available_sensor_columns = [col for col in df.columns if col in recognized_sensor_columns]
            print(f"CSVã«å­˜åœ¨ã™ã‚‹èªè­˜å¯èƒ½ãªåˆ—: {available_sensor_columns}")
            
            if not available_sensor_columns:
                raise HTTPException(
                    status_code=400, 
                    detail=f"èªè­˜å¯èƒ½ãªã‚»ãƒ³ã‚µãƒ¼åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}"
                )
            
            processed = 0
            skipped = 0
            errors = []
            
            for index, row in df.iterrows():
                user_id = str(row.get('user_id', '')).strip()
                
                print(f"å‡¦ç†ä¸­ è¡Œ{index+1}: user_id='{user_id}'")
                
                if not user_id or user_id == 'nan':
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: user_idãŒç©º")
                    continue
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                from app.models.user import User
                user = db.query(User).filter_by(user_id=user_id).first()
                if not user:
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: æœªç™»éŒ²ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}'")
                    print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}' å­˜åœ¨ç¢ºèª")
                
                # å„ã‚»ãƒ³ã‚µãƒ¼ã®ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
                user_mappings_created = 0
                for col_name, sensor_type in recognized_sensor_columns.items():
                    if col_name in df.columns:
                        sensor_id = str(row.get(col_name, '')).strip()
                        print(f"åˆ— '{col_name}': sensor_id='{sensor_id}'")
                        
                        if sensor_id and sensor_id != 'nan' and sensor_id != '':
                            try:
                                # ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                                mapping = FlexibleSensorMapping(
                                    user_id=user_id,
                                    competition_id=competition_id,
                                    sensor_id=sensor_id,
                                    sensor_type=sensor_type,
                                    subject_name=str(row.get('subject_name', '')).strip() or None,
                                    device_type=col_name,
                                    is_active=True,
                                    created_at=datetime.now()
                                )
                                db.add(mapping)
                                user_mappings_created += 1
                                print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ: {user_id} -> {sensor_id} ({sensor_type.value})")
                                
                            except Exception as e:
                                errors.append(f"è¡Œ{index+1}, åˆ—{col_name}: ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆã‚¨ãƒ©ãƒ¼ - {str(e)}")
                                print(f"ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                    else:
                        print(f"åˆ— '{col_name}' ã¯CSVã«å­˜åœ¨ã—ã¾ã›ã‚“")
                
                print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}' ã§ä½œæˆã•ã‚ŒãŸãƒãƒƒãƒ”ãƒ³ã‚°æ•°: {user_mappings_created}")
                
                if user_mappings_created > 0:
                    processed += 1
                else:
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}' ã«æœ‰åŠ¹ãªã‚»ãƒ³ã‚µãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—")
            
            db.commit()
            
            # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = f"ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’{processed}ä»¶ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦å‡¦ç†ã—ã¾ã—ãŸ"
            if skipped > 0:
                message += f"ï¼ˆã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶ï¼‰"
            
            return {
                "success": True,
                "message": message,
                "total_records": len(df),
                "processed_records": processed,
                "skipped_records": skipped,
                "errors": errors[:10] if errors else []  # æœ€åˆã®10ä»¶ã®ã¿
            }
            
        except HTTPException:
            # HTTPExceptionã¯å†ç™ºç”Ÿã•ã›ã‚‹
            raise
        except Exception as e:
            db.rollback()
            error_message = f"ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(error_message)
            
            # dfãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            total_records = len(df) if df is not None else 0
            
            raise HTTPException(status_code=500, detail=error_message)

    async def process_wbgt_data(
        self,
        wbgt_file: UploadFile,
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> UploadResponse:
        """WBGTç’°å¢ƒãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆï¼‹ãƒãƒƒãƒç®¡ç†ï¼‰"""
        try:
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp}_{wbgt_file.filename}"
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            if overwrite and competition_id:
                deleted_count = db.query(WBGTData).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜WBGTãƒ‡ãƒ¼ã‚¿{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
            content = await wbgt_file.read()
            
            # Shift_JISã§ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆWBGTãƒ‡ãƒ¼ã‚¿ã¯æ—¥æœ¬ã®æ©Ÿå™¨ãªã®ã§Shift_JISï¼‰
            try:
                decoded_content = content.decode('shift_jis')
                detected_encoding = 'shift_jis'
            except UnicodeDecodeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                for encoding in ['utf-8', 'cp932', 'iso-8859-1']:
                    try:
                        decoded_content = content.decode(encoding)
                        detected_encoding = encoding
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            print(f"ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
            
            # CSVã‚’DataFrameã«èª­ã¿è¾¼ã¿
            df = pd.read_csv(io.StringIO(decoded_content))
            
            print(f"èª­ã¿è¾¼ã¿å®Œäº† - è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
            print(f"åˆ—å: {list(df.columns)}")
            
            # åˆ—åã®æ­£è¦åŒ–ï¼ˆæ–‡å­—åŒ–ã‘å¯¾å¿œï¼‰
            column_mapping = self._normalize_wbgt_columns(df.columns)
            
            if not column_mapping:
                raise HTTPException(
                    status_code=400, 
                    detail=f"WBGTå¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã®åˆ—: {list(df.columns)}"
                )
            
            print(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°: {column_mapping}")
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed = 0
            errors = []
            
            for index, row in df.iterrows():
                try:
                    # æ—¥ä»˜ã¨æ™‚åˆ»ã®çµåˆ
                    datetime_value = self._combine_date_time(row, column_mapping)
                    
                    if datetime_value is None:
                        errors.append(f"è¡Œ{index+1}: æ—¥ä»˜ãƒ»æ™‚åˆ»ã®çµåˆã«å¤±æ•—")
                        continue
                    
                    # WBGTé–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
                    wbgt_value = self._safe_float(row.get(column_mapping.get('wbgt')))
                    air_temp = self._safe_float(row.get(column_mapping.get('air_temperature')))
                    humidity = self._safe_float(row.get(column_mapping.get('humidity')))
                    globe_temp = self._safe_float(row.get(column_mapping.get('globe_temperature')))
                    
                    # å¿…é ˆé …ç›®ãƒã‚§ãƒƒã‚¯
                    if wbgt_value is None:
                        errors.append(f"è¡Œ{index+1}: WBGTå€¤ãŒç„¡åŠ¹")
                        continue
                    
                    # WBGTDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆï¼ˆãƒãƒƒãƒç®¡ç†å¯¾å¿œï¼‰
                    wbgt_data = WBGTData(
                        timestamp=datetime_value,
                        wbgt_value=wbgt_value,
                        air_temperature=air_temp,
                        humidity=humidity,
                        globe_temperature=globe_temp,
                        competition_id=competition_id,
                        upload_batch_id=batch_id,  # ãƒãƒƒãƒIDè¨­å®š
                        uploaded_at=datetime.now()
                    )
                    
                    db.add(wbgt_data)
                    processed += 1
                    
                except Exception as e:
                    errors.append(f"è¡Œ{index+1}: {str(e)}")
                    continue
            
            # UploadBatchè¨˜éŒ²ä½œæˆ
            from app.models.flexible_sensor_data import UploadBatch, UploadStatus, SensorType
            
            upload_batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.WBGT,
                competition_id=competition_id,
                file_name=wbgt_file.filename,
                file_size=len(content),
                total_records=len(df),
                success_records=processed,
                failed_records=len(errors),
                status=UploadStatus.SUCCESS if len(errors) == 0 else UploadStatus.PARTIAL,
                uploaded_by="admin",  # TODO: å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã«å¤‰æ›´
                notes=f"ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶" if errors else None
            )
            db.add(upload_batch)
            
            db.commit()
            
            # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = f"WBGTãƒ‡ãƒ¼ã‚¿ã‚’{processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸï¼ˆãƒãƒƒãƒID: {batch_id}ï¼‰"
            if errors:
                message += f"ï¼ˆã‚¨ãƒ©ãƒ¼{len(errors)}ä»¶ï¼‰"
            
            return UploadResponse(
                success=True,
                message=message,
                total_records=len(df),
                processed_records=processed,
                success_records=processed,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§ã®ãŸã‚
                failed_records=len(errors),
                errors=errors[:10] if errors else None  # æœ€åˆã®10ä»¶ã®ã¿
            )
            
        except Exception as e:
            db.rollback()
            print(f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise HTTPException(status_code=500, detail=f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def get_data_summary(
        self,
        db: Session,
        competition_id: Optional[str] = None
    ) -> DataSummaryResponse:
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼å–å¾—"""
        
        # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        query = db.query(RawSensorData)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        total_records = query.count()
        mapped_records = query.filter(RawSensorData.mapping_status == SensorDataStatus.MAPPED).count()
        unmapped_records = query.filter(RawSensorData.mapping_status == SensorDataStatus.UNMAPPED).count()
        
        # ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        sensor_counts = {}
        for sensor_type in SensorType:
            count = query.filter(RawSensorData.sensor_type == sensor_type).count()
            sensor_counts[sensor_type.value] = count
        
        wbgt_count = db.query(WBGTData)
        if competition_id:
            wbgt_count = wbgt_count.filter_by(competition_id=competition_id)
        wbgt_count = wbgt_count.count()
        
        mapping_count = db.query(FlexibleSensorMapping)
        if competition_id:
            mapping_count = mapping_count.filter_by(competition_id=competition_id)
        mapping_count = mapping_count.count()
        
        return DataSummaryResponse(
            total_sensor_records=total_records,
            mapped_records=mapped_records,
            unmapped_records=unmapped_records,
            sensor_type_counts=sensor_counts,
            wbgt_records=wbgt_count,
            mapping_records=mapping_count,
            competition_id=competition_id
        )

    def get_mapping_status(
        self,
        db: Session,
        competition_id: Optional[str] = None
    ) -> MappingStatusResponse:
        """ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³ç¢ºèª"""
        
        query = db.query(RawSensorData)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        status_counts = {}
        for status in SensorDataStatus:
            count = query.filter_by(mapping_status=status).count()
            status_counts[status.value] = count
        
        unmapped_by_type = {}
        for sensor_type in SensorType:
            count = query.filter(
                RawSensorData.sensor_type == sensor_type,
                RawSensorData.mapping_status == SensorDataStatus.UNMAPPED
            ).count()
            unmapped_by_type[sensor_type.value] = count
        
        return MappingStatusResponse(
            status_counts=status_counts,
            unmapped_by_sensor_type=unmapped_by_type,
            competition_id=competition_id
        )

    def get_unmapped_sensors(
        self,
        db: Session,
        sensor_type: Optional[SensorType] = None,
        competition_id: Optional[str] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """æœªãƒãƒƒãƒ—ã‚»ãƒ³ã‚µãƒ¼ä¸€è¦§å–å¾—"""
        
        query = db.query(RawSensorData).filter_by(mapping_status=SensorDataStatus.UNMAPPED)
        
        if sensor_type:
            query = query.filter_by(sensor_type=sensor_type)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        # ã‚»ãƒ³ã‚µãƒ¼IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        unmapped_sensors = query.with_entities(
            RawSensorData.sensor_id,
            RawSensorData.sensor_type,
            RawSensorData.competition_id
        ).distinct().limit(limit).all()
        
        return [
            {
                'sensor_id': sensor.sensor_id,
                'sensor_type': sensor.sensor_type.value,
                'competition_id': sensor.competition_id
            }
            for sensor in unmapped_sensors
        ]

    def _detect_race_phases(self, record_data: dict) -> dict:
        """SWIM/BIKE/RUNåŒºé–“è‡ªå‹•åˆ¤å®šï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•èƒŒæ™¯è‰²ç”¨ï¼‰"""
        phases = {
            'swim_phase': None,
            'bike_phase': None, 
            'run_phase': None,
            'total_phase': None,
            'transition_phases': []  # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æœŸé–“
        }
        
        try:
            # å…¨ä½“ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»
            start_times = [record_data['swim_start'], record_data['bike_start'], record_data['run_start']]
            finish_times = [record_data['swim_finish'], record_data['bike_finish'], record_data['run_finish']]
            
            start_times = [t for t in start_times if t is not None]
            finish_times = [t for t in finish_times if t is not None]
            
            if start_times and finish_times:
                total_start = min(start_times)
                total_finish = max(finish_times)
                phases['total_phase'] = {
                    'start': total_start,
                    'finish': total_finish,
                    'duration_seconds': (total_finish - total_start).total_seconds()
                }
            
            # å„ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ã®èƒŒæ™¯è‰²ç”¨ï¼‰
            if record_data['swim_start'] and record_data['swim_finish']:
                swim_duration = (record_data['swim_finish'] - record_data['swim_start']).total_seconds()
                phases['swim_phase'] = {
                    'start': record_data['swim_start'],
                    'finish': record_data['swim_finish'],
                    'duration_seconds': swim_duration,
                    'phase_type': 'swim'
                }
            
            if record_data['bike_start'] and record_data['bike_finish']:
                bike_duration = (record_data['bike_finish'] - record_data['bike_start']).total_seconds()
                phases['bike_phase'] = {
                    'start': record_data['bike_start'],
                    'finish': record_data['bike_finish'],
                    'duration_seconds': bike_duration,
                    'phase_type': 'bike'
                }
            
            if record_data['run_start'] and record_data['run_finish']:
                run_duration = (record_data['run_finish'] - record_data['run_start']).total_seconds()
                phases['run_phase'] = {
                    'start': record_data['run_start'],
                    'finish': record_data['run_finish'],
                    'duration_seconds': run_duration,
                    'phase_type': 'run'
                }
            
            # ğŸ†• ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æœŸé–“ã®æ¤œå‡ºï¼ˆç«¶æŠ€é–“ã®ç§»è¡Œæ™‚é–“ï¼‰
            transitions = []
            
            # SWIM â†’ BIKE ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            if (record_data['swim_finish'] and record_data['bike_start'] and 
                record_data['bike_start'] > record_data['swim_finish']):
                t1_duration = (record_data['bike_start'] - record_data['swim_finish']).total_seconds()
                transitions.append({
                    'name': 'T1_transition',
                    'start': record_data['swim_finish'],
                    'finish': record_data['bike_start'],
                    'duration_seconds': t1_duration,
                    'phase_type': 'transition'
                })
            
            # BIKE â†’ RUN ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            if (record_data['bike_finish'] and record_data['run_start'] and 
                record_data['run_start'] > record_data['bike_finish']):
                t2_duration = (record_data['run_start'] - record_data['bike_finish']).total_seconds()
                transitions.append({
                    'name': 'T2_transition',
                    'start': record_data['bike_finish'],
                    'finish': record_data['run_start'],
                    'duration_seconds': t2_duration,
                    'phase_type': 'transition'
                })
            
            phases['transition_phases'] = transitions
            
            # ğŸ†• LAPæ™‚åˆ»ã®è§£æï¼ˆBL1, BL2...ã‹ã‚‰åŒºé–“æ¨å®šï¼‰
            if record_data['laps']:
                lap_analysis = self._analyze_lap_times(record_data['laps'], phases)
                phases['lap_analysis'] = lap_analysis
            
            # ğŸ†• ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ç”¨ã®æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            phases['graph_segments'] = self._generate_graph_segments(phases)
            
        except Exception as e:
            print(f"åŒºé–“åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚basestructureã¯è¿”ã™
            phases['error'] = str(e)
        
        return phases

    def _analyze_lap_times(self, laps: dict, phases: dict) -> dict:
        """LAPæ™‚åˆ»ã®è§£æï¼ˆBL1, BL2...ã‹ã‚‰ã®è©³ç´°åŒºé–“æ¨å®šï¼‰"""
        lap_analysis = {
            'total_laps': len(laps),
            'lap_times': [],
            'estimated_segments': []
        }
        
        try:
            # LAPæ™‚åˆ»ã‚’ã‚½ãƒ¼ãƒˆ
            sorted_laps = sorted(laps.items(), key=lambda x: x[1] if x[1] else datetime.min)
            
            for i, (lap_name, lap_time) in enumerate(sorted_laps):
                if lap_time is None:
                    continue
                    
                lap_info = {
                    'lap_name': lap_name,
                    'lap_time': lap_time,
                    'lap_number': i + 1
                }
                
                # å‰ã®LAPã¨ã®æ™‚é–“å·®è¨ˆç®—
                if i > 0:
                    prev_time = sorted_laps[i-1][1]
                    if prev_time:
                        interval_seconds = (lap_time - prev_time).total_seconds()
                        lap_info['interval_from_previous'] = interval_seconds
                
                # ç«¶æŠ€é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“
                if phases.get('total_phase') and phases['total_phase'].get('start'):
                    total_start = phases['total_phase']['start']
                    elapsed_seconds = (lap_time - total_start).total_seconds()
                    lap_info['elapsed_from_start'] = elapsed_seconds
                
                lap_analysis['lap_times'].append(lap_info)
            
            # ğŸ†• LAPæ™‚åˆ»ã‹ã‚‰ç«¶æŠ€åŒºé–“ã®æ¨å®š
            lap_analysis['estimated_segments'] = self._estimate_segments_from_laps(
                sorted_laps, phases
            )
            
        except Exception as e:
            lap_analysis['error'] = str(e)
            print(f"LAPè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return lap_analysis

    def _estimate_segments_from_laps(self, sorted_laps, phases: dict) -> list:
        """LAPæ™‚åˆ»ã‹ã‚‰ç«¶æŠ€åŒºé–“ã‚’æ¨å®šï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿BL/RLå¯¾å¿œï¼‰"""
        segments = []
        
        try:
            if not sorted_laps or len(sorted_laps) < 1:
                return segments
            
            # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼šBL(ãƒã‚¤ã‚¯LAP)ã¨RL(ãƒ©ãƒ³LAP)ã®åŒºåˆ¥
            bike_laps = [(name, time) for name, time in sorted_laps if name.upper().startswith('BL')]
            run_laps = [(name, time) for name, time in sorted_laps if name.upper().startswith('RL')]
            
            # ãƒã‚¤ã‚¯LAPåŒºé–“
            if bike_laps:
                bike_laps.sort(key=lambda x: x[1])  # æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆ
                segments.append({
                    'segment_type': 'bike_lap_segment',
                    'start_lap': bike_laps[0][0],
                    'end_lap': bike_laps[-1][0],
                    'start_time': bike_laps[0][1],
                    'end_time': bike_laps[-1][1],
                    'lap_count': len(bike_laps),
                    'confidence': 'high'  # BLåˆ—ãªã®ã§ç¢ºå®Ÿã«ãƒã‚¤ã‚¯åŒºé–“
                })
            
            # ãƒ©ãƒ³LAPåŒºé–“
            if run_laps:
                run_laps.sort(key=lambda x: x[1])  # æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆ
                segments.append({
                    'segment_type': 'run_lap_segment',
                    'start_lap': run_laps[0][0],
                    'end_lap': run_laps[-1][0],
                    'start_time': run_laps[0][1],
                    'end_time': run_laps[-1][1],
                    'lap_count': len(run_laps),
                    'confidence': 'high'  # RLåˆ—ãªã®ã§ç¢ºå®Ÿã«ãƒ©ãƒ³åŒºé–“
                })
            
            # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåŒºé–“æ¨å®šãƒ­ã‚¸ãƒƒã‚¯
            # START â†’ SFï¼šã‚¹ã‚¤ãƒ åŒºé–“
            # SF â†’ BSï¼šç¬¬1ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            # BS â†’ BL1ã€œBLnï¼šãƒã‚¤ã‚¯åŒºé–“
            # BLn â†’ RSï¼šç¬¬2ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ï¼ˆãƒã‚¤ã‚¯çµ‚äº†åˆ¤å®šãŒå¿…è¦ãªå ´åˆï¼‰
            # RS â†’ RL1ã€œRLn â†’ RFï¼šãƒ©ãƒ³åŒºé–“
            
            # ã‚ˆã‚Šè©³ç´°ãªè§£æï¼ˆå®Ÿæ¸¬å€¤ã¨çµ„ã¿åˆã‚ã›ï¼‰
            all_times = [(name, time) for name, time in sorted_laps if time]
            all_times.sort(key=lambda x: x[1])
            
            if all_times:
                segments.append({
                    'segment_type': 'total_lap_coverage',
                    'start_lap': all_times[0][0],
                    'end_lap': all_times[-1][0],
                    'start_time': all_times[0][1],
                    'end_time': all_times[-1][1],
                    'total_laps': len(all_times),
                    'bike_laps': len(bike_laps),
                    'run_laps': len(run_laps)
                })
            
        except Exception as e:
            print(f"å®Ÿãƒ‡ãƒ¼ã‚¿åŒºé–“æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return segments

    def _generate_graph_segments(self, phases: dict) -> list:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ç”¨ã®æ™‚é–“è»¸ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        segments = []
        
        try:
            # ç¢ºå®šåŒºé–“ï¼ˆå®Ÿéš›ã®START/FINISHæ™‚åˆ»ã‹ã‚‰ï¼‰
            for phase_name in ['swim_phase', 'bike_phase', 'run_phase']:
                phase = phases.get(phase_name)
                if phase and phase.get('start') and phase.get('finish'):
                    segments.append({
                        'segment_type': phase['phase_type'],
                        'start_time': phase['start'],
                        'end_time': phase['finish'],
                        'duration_seconds': phase['duration_seconds'],
                        'confidence': 'high',  # å®Ÿæ¸¬å€¤ãªã®ã§é«˜ä¿¡é ¼åº¦
                        'background_color': self._get_phase_color(phase['phase_type'])
                    })
            
            # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³åŒºé–“
            for transition in phases.get('transition_phases', []):
                segments.append({
                    'segment_type': 'transition',
                    'start_time': transition['start'],
                    'end_time': transition['finish'],
                    'duration_seconds': transition['duration_seconds'],
                    'confidence': 'high',
                    'background_color': '#f0f0f0'  # ã‚°ãƒ¬ãƒ¼
                })
            
            # LAPæ¨å®šåŒºé–“ï¼ˆå®Ÿæ¸¬å€¤ãŒãªã„å ´åˆã®è£œå®Œï¼‰
            lap_analysis = phases.get('lap_analysis', {})
            if lap_analysis.get('estimated_segments') and not segments:
                # å®Ÿæ¸¬å€¤ãŒãªã„å ´åˆã®ã¿LAPæ¨å®šã‚’ä½¿ç”¨
                for est_segment in lap_analysis['estimated_segments']:
                    segments.append({
                        'segment_type': est_segment['segment_type'],
                        'start_time': est_segment['start_time'],
                        'end_time': est_segment['end_time'],
                        'confidence': 'medium',  # æ¨å®šå€¤ãªã®ã§ä¸­ä¿¡é ¼åº¦
                        'background_color': self._get_phase_color(
                            est_segment['segment_type'].replace('estimated_', '')
                        )
                    })
            
            # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
            segments.sort(key=lambda x: x['start_time'])
            
        except Exception as e:
            print(f"ã‚°ãƒ©ãƒ•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return segments

    def _get_phase_color(self, phase_type: str) -> str:
        """ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚ºã®èƒŒæ™¯è‰²ã‚’å–å¾—"""
        colors = {
            'swim': '#e3f2fd',    # æ°´è‰²ï¼ˆSWIMï¼‰
            'bike': '#fff3e0',    # ã‚ªãƒ¬ãƒ³ã‚¸ï¼ˆBIKEï¼‰
            'run': '#e8f5e8',     # ç·‘ï¼ˆRUNï¼‰
            'transition': '#f5f5f5'  # ã‚°ãƒ¬ãƒ¼ï¼ˆãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ï¼‰
        }
        return colors.get(phase_type, '#ffffff')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç™½# app/services/flexible_csv_service.py ã«è¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰

    async def process_race_record_data(
        self,
        race_files: List[UploadFile],
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> Dict[str, Any]:
        """
        å¤§ä¼šè¨˜éŒ²ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆè¤‡æ•°CSVçµ±åˆå¯¾å¿œï¼‰
        
        ä»•æ§˜æ›¸2.5æº–æ‹ :
        - è¤‡æ•°CSVãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ
        - ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ï¼ˆ"No."åˆ—ï¼‰ã«ã‚ˆã‚‹çµ±åˆ
        - å¯å¤‰LAPæ§‹æˆï¼ˆBL1, BL2...ï¼‰å¯¾å¿œ
        - SWIM/BIKE/RUNåŒºé–“è‡ªå‹•åˆ¤å®š
        """
        try:
            # å¤§ä¼šå­˜åœ¨ãƒã‚§ãƒƒã‚¯
            from app.models.competition import Competition, RaceRecord
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜å¤§ä¼šè¨˜éŒ²ã‚’å‰Šé™¤
            if overwrite and competition_id:
                deleted_count = db.query(RaceRecord).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜å¤§ä¼šè¨˜éŒ²{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp}_race_records_{len(race_files)}files"
            
            # å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ
            all_records = {}  # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ã‚’ã‚­ãƒ¼ã¨ã—ãŸçµ±åˆãƒ‡ãƒ¼ã‚¿
            lap_columns = set()  # æ¤œå‡ºã•ã‚ŒãŸLAPåˆ—å
            total_files_processed = 0
            total_csv_records = 0
            errors = []
            
            for file_idx, file in enumerate(race_files):
                try:
                    print(f"Processing file {file_idx + 1}/{len(race_files)}: {file.filename}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    content = await file.read()
                    if not content:
                        errors.append(f"{file.filename}: ç©ºãƒ•ã‚¡ã‚¤ãƒ«")
                        continue
                    
                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
                    detected_encoding = None
                    for encoding in ['utf-8', 'shift_jis', 'cp932', 'iso-8859-1']:
                        try:
                            decoded_content = content.decode(encoding)
                            detected_encoding = encoding
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if detected_encoding is None:
                        errors.append(f"{file.filename}: æ–‡å­—ã‚³ãƒ¼ãƒ‰èªè­˜å¤±æ•—")
                        continue
                    
                    print(f"  ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
                    
                    # CSVã‚’DataFrameã«èª­ã¿è¾¼ã¿
                    df = pd.read_csv(io.StringIO(decoded_content))
                    
                    if df.empty:
                        errors.append(f"{file.filename}: ãƒ‡ãƒ¼ã‚¿ãŒç©º")
                        continue
                    
                    total_csv_records += len(df)
                    
                    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼šã‚¼ãƒƒã‚±ãƒ³ç•ªå·ï¼ˆ"No."åˆ—ï¼‰
                    bib_number_col = None
                    for col in df.columns:
                        if col.strip().lower() in ['no.', 'no', 'bib', 'bib_number', 'ã‚¼ãƒƒã‚±ãƒ³', 'ã‚¼ãƒƒã‚±ãƒ³ç•ªå·']:
                            bib_number_col = col
                            break
                    
                    if bib_number_col is None:
                        errors.append(f"{file.filename}: ã‚¼ãƒƒã‚±ãƒ³ç•ªå·åˆ—ï¼ˆ'No.'ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # LAPåˆ—ã®æ¤œå‡ºï¼ˆBL1, BL2, BL3...ç­‰ + RL1, RL2...ç­‰ï¼‰
                    file_lap_columns = []
                    for col in df.columns:
                        col_upper = col.strip().upper()
                        # ãƒã‚¤ã‚¯LAPï¼ˆBL1, BL2...ï¼‰ã¨ãƒ©ãƒ³LAPï¼ˆRL1, RL2...ï¼‰ä¸¡æ–¹ã‚’æ¤œå‡º
                        if ((col_upper.startswith('BL') or col_upper.startswith('RL')) and 
                            len(col_upper) >= 3 and col_upper[2:].isdigit()):
                            file_lap_columns.append(col)
                            lap_columns.add(col)
                    
                    print(f"  æ¤œå‡ºã•ã‚ŒãŸLAPåˆ—: {file_lap_columns}")
                    
                    # å„è¡Œã‚’å‡¦ç†ã—ã¦ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ã§çµ±åˆ
                    for _, row in df.iterrows():
                        bib_number = str(row[bib_number_col]).strip()
                        
                        if not bib_number or bib_number == 'nan':
                            continue
                        
                        # çµ±åˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®åˆæœŸåŒ–
                        if bib_number not in all_records:
                            all_records[bib_number] = {
                                'bib_number': bib_number,
                                'swim_start': None,
                                'swim_finish': None,
                                'bike_start': None,
                                'bike_finish': None,
                                'run_start': None,
                                'run_finish': None,
                                'laps': {},
                                'source_files': []
                            }
                        
                        record = all_records[bib_number]
                        record['source_files'].append(file.filename)
                        
                        # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼šçŸ­ç¸®å½¢å¼ã®åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
                        for col in df.columns:
                            col_clean = col.strip()
                            col_upper = col_clean.upper()
                            value = row[col]
                            
                            if pd.isna(value):
                                continue
                            
                            # æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿ã®è§£æ
                            time_value = self._parse_race_time(value)
                            if time_value is None:
                                continue
                            
                            # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿ã®çŸ­ç¸®å½¢å¼ã«å¯¾å¿œ
                            if col_upper == 'START':
                                record['swim_start'] = time_value
                            elif col_upper == 'SF':  # Swim Finish
                                record['swim_finish'] = time_value
                            elif col_upper == 'BS':  # Bike Start
                                record['bike_start'] = time_value
                            elif col_upper == 'RS':  # Run Start
                                record['run_start'] = time_value
                            elif col_upper == 'RF':  # Run Finish
                                record['run_finish'] = time_value
                            
                            # ãƒ¬ã‚¬ã‚·ãƒ¼åˆ—åã‚‚å¿µã®ãŸã‚å¯¾å¿œ
                            elif 'swim' in col_upper and 'start' in col_upper:
                                record['swim_start'] = time_value
                            elif 'swim' in col_upper and ('finish' in col_upper or 'end' in col_upper):
                                record['swim_finish'] = time_value
                            elif 'bike' in col_upper and 'start' in col_upper:
                                record['bike_start'] = time_value
                            elif 'bike' in col_upper and ('finish' in col_upper or 'end' in col_upper):
                                record['bike_finish'] = time_value
                            elif 'run' in col_upper and 'start' in col_upper:
                                record['run_start'] = time_value
                            elif 'run' in col_upper and ('finish' in col_upper or 'end' in col_upper):
                                record['run_finish'] = time_value
                        
                        # LAP ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                        for lap_col in file_lap_columns:
                            lap_value = row[lap_col]
                            if not pd.isna(lap_value):
                                lap_time = self._parse_race_time(lap_value)
                                if lap_time:
                                    record['laps'][lap_col] = lap_time
                    
                    total_files_processed += 1
                    print(f"  âœ… {file.filename}: {len(df)}ä»¶ã®è¨˜éŒ²ã‚’å‡¦ç†")
                    
                except Exception as e:
                    error_msg = f"{file.filename}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {str(e)}"
                    errors.append(error_msg)
                    print(f"  âŒ {error_msg}")
                    continue
            
            if not all_records:
                raise HTTPException(status_code=400, detail="å‡¦ç†å¯èƒ½ãªå¤§ä¼šè¨˜éŒ²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            saved_count = 0
            failed_count = 0
            
            for bib_number, record_data in all_records.items():
                try:
                    # SWIM/BIKE/RUNåŒºé–“ã®è‡ªå‹•åˆ¤å®š
                    phases = self._detect_race_phases(record_data)
                    
                    # RaceRecordã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä½œæˆ
                    race_record = RaceRecord(
                        competition_id=competition_id,
                        user_id=None,  # ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã«è¨­å®š
                        race_number=bib_number,
                        swim_start_time=record_data['swim_start'],
                        swim_finish_time=record_data['swim_finish'],
                        bike_start_time=record_data['bike_start'],
                        bike_finish_time=record_data['bike_finish'],
                        run_start_time=record_data['run_start'],
                        run_finish_time=record_data['run_finish'],
                        notes=f"LAPæ•°: {len(record_data['laps'])}, ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(record_data['source_files'])}"
                    )
                    
                    # ğŸ†• LAP ãƒ‡ãƒ¼ã‚¿ã¨åŒºé–“æƒ…å ±ã‚’è¨­å®š
                    if record_data['laps']:
                        race_record.set_lap_data(record_data['laps'])
                    
                    if phases:
                        race_record.set_calculated_phases(phases)
                    
                    db.add(race_record)
                    saved_count += 1
                    
                except Exception as e:
                    failed_count += 1
                    error_msg = f"ã‚¼ãƒƒã‚±ãƒ³{bib_number}: ä¿å­˜ã‚¨ãƒ©ãƒ¼ - {str(e)}"
                    errors.append(error_msg)
                    print(f"âŒ {error_msg}")
            
            # ãƒãƒƒãƒè¨˜éŒ²ä½œæˆ
            from app.models.flexible_sensor_data import UploadBatch, SensorType, UploadStatus
            
            total_file_size = sum([len(await f.read()) for f in race_files])
            
            batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.OTHER,  # å¤§ä¼šè¨˜éŒ²ç”¨
                competition_id=competition_id,
                file_name=f"race_records_{len(race_files)}files.csv",
                file_size=total_file_size,
                total_records=total_csv_records,
                success_records=saved_count,
                failed_records=failed_count,
                status=UploadStatus.SUCCESS if failed_count == 0 else UploadStatus.PARTIAL,
                uploaded_by=db.query(AdminUser).first().admin_id,  # è¦ä¿®æ­£
                notes=f"çµ±åˆLAPåˆ—: {', '.join(sorted(lap_columns))}" if lap_columns else None
            )
            db.add(batch)
            
            db.commit()
            
            # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = f"å¤§ä¼šè¨˜éŒ²ã‚’{saved_count}ä»¶çµ±åˆå‡¦ç†ã—ã¾ã—ãŸ"
            if failed_count > 0:
                message += f"ï¼ˆå¤±æ•—: {failed_count}ä»¶ï¼‰"
            
            return {
                "success": saved_count > 0,
                "message": message,
                "total_files": len(race_files),
                "processed_files": total_files_processed,
                "total_csv_records": total_csv_records,
                "saved_records": saved_count,
                "failed_records": failed_count,
                "detected_lap_columns": sorted(lap_columns),
                "batch_id": batch_id,
                "errors": errors[:10] if errors else []  # æœ€åˆã®10ä»¶ã®ã¿
            }
            
        except HTTPException:
            raise
        except Exception as e:
            db.rollback()
            error_message = f"å¤§ä¼šè¨˜éŒ²å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(error_message)
            raise HTTPException(status_code=500, detail=error_message)

    def _parse_race_time(self, time_value) -> Optional[datetime]:
        """ãƒ¬ãƒ¼ã‚¹æ™‚åˆ»ã®æŸ”è»Ÿãªè§£æ"""
        if pd.isna(time_value):
            return None
        
        time_str = str(time_value).strip()
        if not time_str or time_str.lower() in ['nan', '', 'null']:
            return None
        
        try:
            # è¤‡æ•°ã®æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M:%S.%f",
                "%Y/%m/%d %H:%M:%S",
                "%d/%m/%Y %H:%M:%S",
                "%H:%M:%S",
                "%H:%M",
                "%Y-%m-%d",
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(time_str, fmt)
                except ValueError:
                    continue
            
            # pandas to_datetimeã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return pd.to_datetime(time_str)
            
        except Exception as e:
            print(f"æ™‚åˆ»è§£æã‚¨ãƒ©ãƒ¼: {time_str} - {e}")
            return None

    def _detect_race_phases(self, record_data: dict) -> dict:
        """SWIM/BIKE/RUNåŒºé–“è‡ªå‹•åˆ¤å®š"""
        phases = {
            'swim_phase': None,
            'bike_phase': None, 
            'run_phase': None,
            'total_phase': None
        }
        
        try:
            # å…¨ä½“ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»
            start_times = [record_data['swim_start'], record_data['bike_start'], record_data['run_start']]
            finish_times = [record_data['swim_finish'], record_data['bike_finish'], record_data['run_finish']]
            
            start_times = [t for t in start_times if t is not None]
            finish_times = [t for t in finish_times if t is not None]
            
            if start_times and finish_times:
                total_start = min(start_times)
                total_finish = max(finish_times)
                phases['total_phase'] = {'start': total_start, 'finish': total_finish}
            
            # å„ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚º
            if record_data['swim_start'] and record_data['swim_finish']:
                phases['swim_phase'] = {
                    'start': record_data['swim_start'],
                    'finish': record_data['swim_finish']
                }
            
            if record_data['bike_start'] and record_data['bike_finish']:
                phases['bike_phase'] = {
                    'start': record_data['bike_start'],
                    'finish': record_data['bike_finish']
                }
            
            if record_data['run_start'] and record_data['run_finish']:
                phases['run_phase'] = {
                    'start': record_data['run_start'],
                    'finish': record_data['run_finish']
                }
            
        except Exception as e:
            print(f"åŒºé–“åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return phases