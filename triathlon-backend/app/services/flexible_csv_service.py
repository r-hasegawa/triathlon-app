"""
app/services/flexible_csv_service.py

å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆï¼ˆWBGTå®Ÿè£…å«ã‚€ï¼‰
"""

import pandas as pd
import io
from io import BytesIO
import chardet
from fastapi import UploadFile, HTTPException
from sqlalchemy.orm import Session
from typing import Optional, Dict, Any, List
from datetime import datetime
from app.models.competition import Competition, RaceRecord
from app.models.user import AdminUser
from app.models.flexible_sensor_data import (
    RawSensorData, FlexibleSensorMapping,
    SkinTemperatureData, CoreTemperatureData, 
    HeartRateData, WBGTData, SensorDataStatus, SensorType,
    UploadBatch, SensorType, UploadStatus
)
from app.schemas.sensor_data import (
    UploadResponse, MappingResponse,
    DataSummaryResponse, MappingStatusResponse
)

class FlexibleCSVService:

    def _detect_encoding(self, content: bytes) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
        result = chardet.detect(content)
        encoding = result.get('encoding', 'utf-8')
        confidence = result.get('confidence', 0)
        
        print(f"æ–‡å­—ã‚³ãƒ¼ãƒ‰æ¤œå‡º: {encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®æ­£è¦åŒ–
        if encoding and encoding.lower() in ['shift_jis', 'shift-jis', 'cp932']:
            return 'shift_jis'
        elif encoding and encoding.lower() in ['utf-8-sig', 'utf-8']:
            return 'utf-8'
        elif encoding and encoding.lower() in ['cp1252', 'iso-8859-1']:
            return 'cp1252'
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯UTF-8
            return 'utf-8'
    
    async def process_sensor_data_only(
        self,
        sensor_file: UploadFile,
        sensor_type: SensorType,
        competition_id: str,
        db: Session
    ) -> UploadResponse:
        """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿å‡¦ç†"""
        try:
            # å¤§ä¼šå­˜åœ¨ãƒã‚§ãƒƒã‚¯è¿½åŠ 
            from app.models import Competition
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            content = await sensor_file.read()
            df = pd.read_csv(io.BytesIO(content))
            
            processed = 0
            
            for _, row in df.iterrows():
                raw_data = RawSensorData(
                    sensor_type=sensor_type,
                    sensor_id=str(row.get('sensor_id', '')),
                    timestamp=pd.to_datetime(row.get('timestamp')),
                    data_values=str(row.get('value', 0)),
                    competition_id=competition_id,
                    mapping_status=SensorDataStatus.UNMAPPED,
                    raw_data=row.to_json()
                )
                db.add(raw_data)
                processed += 1
            
            db.commit()
            
            return UploadResponse(
                success=True,
                message=f"{sensor_type.value}ãƒ‡ãƒ¼ã‚¿ã‚’{processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸï¼ˆå¤§ä¼š: {competition.name}ï¼‰",
                total_records=len(df),
                processed_records=processed
            )
            
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=400, detail=f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    async def process_wbgt_data(
        self,
        wbgt_file: UploadFile,
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> UploadResponse:
        """WBGTç’°å¢ƒãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆè¨ˆæ¸¬æ™‚åˆ»ã‚’timestampã«ä¿å­˜ï¼‰"""
        try:
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp_str}_{wbgt_file.filename}"
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
            if overwrite:
                deleted_count = db.query(WBGTData).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜WBGTãƒ‡ãƒ¼ã‚¿{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # CSVèª­ã¿è¾¼ã¿
            content = await wbgt_file.read()
            decoded_content = content.decode('shift_jis')
            df = pd.read_csv(io.StringIO(decoded_content))
            
            # åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°
            column_mapping = {
                'date': 'æ—¥ä»˜',
                'time': 'æ™‚åˆ»',
                'wbgt': 'WBGTå€¤',
                'air_temperature': 'æ°—æ¸©',
                'humidity': 'ç›¸å¯¾æ¹¿åº¦',
                'globe_temperature': 'é»’çƒæ¸©åº¦'
            }
            
            processed = 0
            errors = []
            
            for idx, row in df.iterrows():
                try:
                    # è¨ˆæ¸¬æ™‚åˆ»ã‚’ timestamp ã«
                    date_str = str(row[column_mapping['date']]).strip()
                    time_str = str(row[column_mapping['time']]).strip()
                    dt = datetime.strptime(f"{date_str} {time_str}", '%Y/%m/%d %H:%M:%S')
                    
                    wbgt_value = float(row[column_mapping['wbgt']])
                    air_temp = float(row[column_mapping['air_temperature']])
                    humidity = float(row[column_mapping['humidity']])
                    globe_temp = float(row[column_mapping['globe_temperature']])
                    
                    wbgt_data = WBGTData(
                        timestamp=dt,
                        wbgt_value=wbgt_value,
                        air_temperature=air_temp,
                        humidity=humidity,
                        globe_temperature=globe_temp,
                        competition_id=competition_id,
                        upload_batch_id=batch_id
                    )
                    db.add(wbgt_data)
                    processed += 1
                except Exception as e:
                    errors.append(f"è¡Œ{idx+1}: {e}")
                    continue
            
            # UploadBatchç™»éŒ²
            from app.models.flexible_sensor_data import UploadBatch, UploadStatus, SensorType
            batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.WBGT,
                competition_id=competition_id,
                file_name=wbgt_file.filename,
                file_size=len(content),
                total_records=len(df),
                success_records=processed,
                failed_records=len(errors),
                status=UploadStatus.SUCCESS if not errors else UploadStatus.PARTIAL,
                uploaded_by="admin",
                notes=f"ã‚¨ãƒ©ãƒ¼{len(errors)}ä»¶" if errors else None
            )
            db.add(batch)
            db.commit()
            
            return UploadResponse(
                success=True,
                message=f"WBGTãƒ‡ãƒ¼ã‚¿ {processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸ",
                total_records=len(df),
                processed_records=processed,
                errors=errors[:20]  # æœ€å¤§20ä»¶
            )
            
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=400, detail=f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def _normalize_wbgt_columns(self, columns: List[str]) -> Dict[str, str]:
        """WBGTåˆ—åã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        columns = [str(col).strip() for col in columns]
        mapping = {}
        
        # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªåˆ—åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        column_map = {
            'æ—¥ä»˜': 'date',
            'æ™‚åˆ»': 'time', 
            'WBGTå€¤': 'wbgt',
            'æ°—æ¸©': 'air_temperature',
            'ç›¸å¯¾æ¹¿åº¦': 'humidity',
            'é»’çƒæ¸©åº¦': 'globe_temperature'
        }
        
        # æ­£ç¢ºãªåˆ—åãƒãƒƒãƒãƒ³ã‚°
        for col in columns:
            if col in column_map:
                mapping[column_map[col]] = col
        
        # æœ€ä½é™WBGTå€¤ãŒå¿…è¦
        if 'wbgt' not in mapping:
            return {}
        
        return mapping

    def _combine_date_time(self, row: pd.Series, column_mapping: Dict[str, str]) -> Optional[datetime]:
        """æ—¥ä»˜ã¨æ™‚åˆ»ã‚’çµåˆã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        try:
            date_col = column_mapping.get('date')  # 'æ—¥ä»˜'
            time_col = column_mapping.get('time')  # 'æ™‚åˆ»'
            
            if not date_col or not time_col:
                return None
                
            date_str = str(row.get(date_col, '')).strip()  # '2025/07/15'
            time_str = str(row.get(time_col, '')).strip()  # '17:43:38'
            
            if not date_str or not time_str or date_str == 'nan' or time_str == 'nan':
                return None
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: '2025/07/15 17:43:38'
            datetime_str = f"{date_str} {time_str}"
            
            try:
                # å®Ÿãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                return datetime.strptime(datetime_str, '%Y/%m/%d %H:%M:%S')
            except ValueError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    return pd.to_datetime(datetime_str)
                except:
                    return None
            
        except Exception as e:
            print(f"æ—¥ä»˜ãƒ»æ™‚åˆ»çµåˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨ã«floatå¤‰æ›"""
        if value is None or pd.isna(value):
            return None
        
        try:
            return float(value)
        except (ValueError, TypeError):
            return None

    async def process_mapping_data(
        self, 
        mapping_file: UploadFile, 
        competition_id: str, 
        db: Session, 
        overwrite: bool = True
    ) -> dict:
        """
        ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰
        
        æ‹¡å¼µCSVæ§‹é€ :
        user_id,race_number,skin_temp_sensor_id,core_temp_sensor_id,heart_rate_sensor_id,subject_name
        """
        
        try:
            # å¤§ä¼šå­˜åœ¨ãƒã‚§ãƒƒã‚¯  
            from app.models.competition import Competition
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # æ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°å‰Šé™¤ï¼ˆoverwriteãŒæœ‰åŠ¹ãªå ´åˆï¼‰
            if overwrite:
                existing_count = db.query(FlexibleSensorMapping).filter_by(competition_id=competition_id).delete()
                print(f"æ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°å‰Šé™¤: {existing_count}ä»¶")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
            content = await mapping_file.read()
            encoding = self._detect_encoding(content)
            
            try:
                df = pd.read_csv(BytesIO(content), encoding=encoding)
                print(f"CSVèª­ã¿è¾¼ã¿æˆåŠŸ: {encoding}")
            except UnicodeDecodeError as e:
                print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼({encoding}): {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                try:
                    df = pd.read_csv(BytesIO(content), encoding='utf-8')
                    print("UTF-8ã§å†è©¦è¡ŒæˆåŠŸ")
                except Exception as e2:
                    try:
                        df = pd.read_csv(BytesIO(content), encoding='shift_jis')
                        print("Shift-JISã§å†è©¦è¡ŒæˆåŠŸ")
                    except Exception as e3:
                        raise HTTPException(status_code=400, detail=f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {str(e3)}")
            
            if df.empty:
                raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
            
            # åˆ—åã®ç©ºç™½é™¤å»ã¨æ­£è¦åŒ–
            original_columns = list(df.columns)
            df.columns = df.columns.str.strip()
            cleaned_columns = list(df.columns)
            
            print(f"ğŸ“Š CSVæƒ…å ±:")
            print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«å: {mapping_file.filename}")
            print(f"   - è¡Œæ•°: {len(df)}")
            print(f"   - å…ƒã®åˆ—å: {original_columns}")
            print(f"   - æ­£è¦åŒ–å¾Œ: {cleaned_columns}")
            
            # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
            if 'user_id' not in df.columns:
                raise HTTPException(status_code=400, detail="user_idåˆ—ãŒå¿…è¦ã§ã™")
            
            # user_idé‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicate_users = df[df['user_id'].duplicated()]['user_id'].tolist()
            if duplicate_users:
                raise HTTPException(
                    status_code=400, 
                    detail=f"user_idã«é‡è¤‡ãŒã‚ã‚Šã¾ã™: {duplicate_users}"
                )
            
            # èªè­˜ã™ã‚‹ã‚»ãƒ³ã‚µãƒ¼åˆ—
            recognized_sensor_columns = {
                'skin_temp_sensor_id': SensorType.SKIN_TEMPERATURE,
                'core_temp_sensor_id': SensorType.CORE_TEMPERATURE,
                'heart_rate_sensor_id': SensorType.HEART_RATE,
                'skin_temperature_sensor_id': SensorType.SKIN_TEMPERATURE,
                'core_temperature_sensor_id': SensorType.CORE_TEMPERATURE,
                'heart_rate_id': SensorType.HEART_RATE,
            }
            
            available_sensor_columns = [col for col in df.columns if col in recognized_sensor_columns]
            has_race_number = 'race_number' in df.columns
            
            print(f"ğŸ“Š èªè­˜å¯èƒ½ãªåˆ—: {available_sensor_columns}")
            print(f"ğŸ“Š ã‚¼ãƒƒã‚±ãƒ³ç•ªå·åˆ—: {has_race_number}")
            
            if not available_sensor_columns and not has_race_number:
                raise HTTPException(
                    status_code=400, 
                    detail=f"èªè­˜å¯èƒ½ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¿…è¦: user_id, ã‚»ãƒ³ã‚µãƒ¼IDåˆ—ã¾ãŸã¯race_numberåˆ—"
                )
            
            processed = 0
            skipped = 0
            errors = []
            race_number_mappings = 0
            
            for index, row in df.iterrows():
                user_id = str(row.get('user_id', '')).strip()
                
                if not user_id or user_id == 'nan':
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: user_idãŒç©º")
                    continue
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                from app.models.user import User
                user = db.query(User).filter_by(user_id=user_id).first()
                if not user:
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: æœªç™»éŒ²ãƒ¦ãƒ¼ã‚¶ãƒ¼ '{user_id}'")
                    continue
                
                user_mappings_created = 0
                
                # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
                if has_race_number:
                    race_number = str(row.get('race_number', '')).strip()
                    if race_number and race_number != 'nan':
                        try:
                            race_mapping = FlexibleSensorMapping(
                                user_id=user_id,
                                competition_id=competition_id,
                                sensor_id=race_number,
                                sensor_type=SensorType.OTHER,
                                race_number=race_number,
                                subject_name=str(row.get('subject_name', '')).strip() or None,
                                device_type='race_number',
                                is_active=True,
                                created_at=datetime.now()
                            )
                            db.add(race_mapping)
                            user_mappings_created += 1
                            race_number_mappings += 1
                        except Exception as e:
                            errors.append(f"è¡Œ{index+1}: ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ - {str(e)}")
                
                # ã‚»ãƒ³ã‚µãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
                for col_name in available_sensor_columns:
                    sensor_id_raw = row.get(col_name)
                    
                    if pd.notna(sensor_id_raw):
                        sensor_id = str(sensor_id_raw).strip()
                        if sensor_id and sensor_id != 'nan':
                            try:
                                sensor_mapping = FlexibleSensorMapping(
                                    user_id=user_id,
                                    competition_id=competition_id,
                                    sensor_id=sensor_id,
                                    sensor_type=recognized_sensor_columns[col_name],
                                    subject_name=str(row.get('subject_name', '')).strip() or None,
                                    is_active=True,
                                    created_at=datetime.now()
                                )
                                db.add(sensor_mapping)
                                user_mappings_created += 1
                            except Exception as e:
                                errors.append(f"è¡Œ{index+1}: ã‚»ãƒ³ã‚µãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼({col_name}) - {str(e)}")
                
                if user_mappings_created > 0:
                    processed += 1
                    print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id}: {user_mappings_created}ä»¶ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ")
                else:
                    skipped += 1
                    errors.append(f"è¡Œ{index+1}: ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãªã— (user_id: {user_id})")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒŸãƒƒãƒˆ
            try:
                db.commit()
                print(f"ğŸ“Š ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†å®Œäº†: å‡¦ç†{processed}ä»¶, ã‚¹ã‚­ãƒƒãƒ—{skipped}ä»¶")
            except Exception as e:
                db.rollback()
                raise HTTPException(status_code=500, detail=f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            return {
                "success": processed > 0,
                "message": f"ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: å‡¦ç†{processed}ä»¶, ã‚¹ã‚­ãƒƒãƒ—{skipped}ä»¶",
                "total_records": len(df),
                "processed_records": processed,
                "skipped_records": skipped,
                "race_number_mappings": race_number_mappings,
                "errors": errors[:20] if errors else []  # æœ€åˆã®20ã‚¨ãƒ©ãƒ¼ã®ã¿
            }
            
        except HTTPException:
            # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
            raise
        except Exception as e:
            db.rollback()
            error_message = f"ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(f"âŒ {error_message}")
            return {
                "success": False,
                "message": error_message,
                "total_records": 0,
                "processed_records": 0,
                "skipped_records": 0,
                "errors": [error_message]
            }

    # === ğŸ†• ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨å‡¦ç†ã®æ‹¡å¼µ ===
    async def apply_race_number_mapping(
        self,
        competition_id: str,
        db: Session
    ) -> dict:
        """
        ğŸ†• ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨
        FlexibleSensorMappingã®race_numberæƒ…å ±ã‚’ä½¿ã£ã¦ã€
        RaceRecordã®user_idã‚’è¨­å®šã™ã‚‹
        """
        
        try:
            from app.models.competition import RaceRecord
            from app.models.flexible_sensor_data import FlexibleSensorMapping
            
            # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°å–å¾—
            race_mappings = db.query(FlexibleSensorMapping).filter_by(
                competition_id=competition_id,
                device_type='race_number',
                is_active=True
            ).all()
            
            print(f"ğŸƒ ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°æ•°: {len(race_mappings)}")
            
            applied_count = 0
            errors = []
            
            for mapping in race_mappings:
                race_number = mapping.race_number
                user_id = mapping.user_id
                
                # å¯¾å¿œã™ã‚‹å¤§ä¼šè¨˜éŒ²ã‚’æ¤œç´¢ãƒ»æ›´æ–°
                race_records = db.query(RaceRecord).filter_by(
                    competition_id=competition_id,
                    race_number=race_number,
                    user_id=None  # æœªãƒãƒƒãƒ”ãƒ³ã‚°ã®ã‚‚ã®ã®ã¿
                ).all()
                
                for record in race_records:
                    record.user_id = user_id
                    applied_count += 1
                    print(f"âœ… å¤§ä¼šè¨˜éŒ²ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨: ã‚¼ãƒƒã‚±ãƒ³{race_number} -> {user_id}")
            
            db.commit()
            
            return {
                "success": True,
                "message": f"ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’{applied_count}ä»¶ã®å¤§ä¼šè¨˜éŒ²ã«é©ç”¨ã—ã¾ã—ãŸ",
                "applied_race_records": applied_count,
                "total_mappings": len(race_mappings)
            }
            
        except Exception as e:
            db.rollback()
            error_message = f"ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(f"âŒ {error_message}")
            
            return {
                "success": False, 
                "message": error_message,
                "applied_race_records": 0,
                "errors": [error_message]
            }

    async def process_wbgt_data(
        self,
        wbgt_file: UploadFile,
        competition_id: str,
        db: Session,
        overwrite: bool = True
    ) -> UploadResponse:
        """WBGTç’°å¢ƒãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œç‰ˆï¼‹ãƒãƒƒãƒç®¡ç†ï¼‰"""
        try:
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp}_{wbgt_file.filename}"
            
            # ä¸Šæ›¸ãå‡¦ç†ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            if overwrite and competition_id:
                deleted_count = db.query(WBGTData).filter_by(competition_id=competition_id).delete()
                db.commit()
                print(f"æ—¢å­˜WBGTãƒ‡ãƒ¼ã‚¿{deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
            content = await wbgt_file.read()
            
            # Shift_JISã§ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆWBGTãƒ‡ãƒ¼ã‚¿ã¯æ—¥æœ¬ã®æ©Ÿå™¨ãªã®ã§Shift_JISï¼‰
            try:
                decoded_content = content.decode('shift_jis')
                detected_encoding = 'shift_jis'
            except UnicodeDecodeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                for encoding in ['utf-8', 'cp932', 'iso-8859-1']:
                    try:
                        decoded_content = content.decode(encoding)
                        detected_encoding = encoding
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise HTTPException(status_code=400, detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            print(f"ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
            
            # CSVã‚’DataFrameã«èª­ã¿è¾¼ã¿
            df = pd.read_csv(io.StringIO(decoded_content))
            
            print(f"èª­ã¿è¾¼ã¿å®Œäº† - è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
            print(f"åˆ—å: {list(df.columns)}")
            
            # åˆ—åã®æ­£è¦åŒ–ï¼ˆæ–‡å­—åŒ–ã‘å¯¾å¿œï¼‰
            column_mapping = self._normalize_wbgt_columns(df.columns)
            
            if not column_mapping:
                raise HTTPException(
                    status_code=400, 
                    detail=f"WBGTå¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã®åˆ—: {list(df.columns)}"
                )
            
            print(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°: {column_mapping}")
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed = 0
            errors = []
            
            for index, row in df.iterrows():
                try:
                    # æ—¥ä»˜ã¨æ™‚åˆ»ã®çµåˆ
                    datetime_value = self._combine_date_time(row, column_mapping)
                    
                    if datetime_value is None:
                        errors.append(f"è¡Œ{index+1}: æ—¥ä»˜ãƒ»æ™‚åˆ»ã®çµåˆã«å¤±æ•—")
                        continue
                    
                    # WBGTé–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
                    wbgt_value = self._safe_float(row.get(column_mapping.get('wbgt')))
                    air_temp = self._safe_float(row.get(column_mapping.get('air_temperature')))
                    humidity = self._safe_float(row.get(column_mapping.get('humidity')))
                    globe_temp = self._safe_float(row.get(column_mapping.get('globe_temperature')))
                    
                    # å¿…é ˆé …ç›®ãƒã‚§ãƒƒã‚¯
                    if wbgt_value is None:
                        errors.append(f"è¡Œ{index+1}: WBGTå€¤ãŒç„¡åŠ¹")
                        continue
                    
                    # WBGTDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆï¼ˆãƒãƒƒãƒç®¡ç†å¯¾å¿œï¼‰
                    wbgt_data = WBGTData(
                        timestamp=datetime_value,
                        wbgt_value=wbgt_value,
                        air_temperature=air_temp,
                        humidity=humidity,
                        globe_temperature=globe_temp,
                        competition_id=competition_id,
                        upload_batch_id=batch_id,  # ãƒãƒƒãƒIDè¨­å®š
                        uploaded_at=datetime.now()
                    )
                    
                    db.add(wbgt_data)
                    processed += 1
                    
                except Exception as e:
                    errors.append(f"è¡Œ{index+1}: {str(e)}")
                    continue
            
            # UploadBatchè¨˜éŒ²ä½œæˆ
            from app.models.flexible_sensor_data import UploadBatch, UploadStatus, SensorType
            
            upload_batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.WBGT,
                competition_id=competition_id,
                file_name=wbgt_file.filename,
                file_size=len(content),
                total_records=len(df),
                success_records=processed,
                failed_records=len(errors),
                status=UploadStatus.SUCCESS if len(errors) == 0 else UploadStatus.PARTIAL,
                uploaded_by="admin",  # TODO: å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã«å¤‰æ›´
                notes=f"ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶" if errors else None
            )
            db.add(upload_batch)
            
            db.commit()
            
            # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = f"WBGTãƒ‡ãƒ¼ã‚¿ã‚’{processed}ä»¶å‡¦ç†ã—ã¾ã—ãŸï¼ˆãƒãƒƒãƒID: {batch_id}ï¼‰"
            if errors:
                message += f"ï¼ˆã‚¨ãƒ©ãƒ¼{len(errors)}ä»¶ï¼‰"
            
            return UploadResponse(
                success=True,
                message=message,
                total_records=len(df),
                processed_records=processed,
                success_records=processed,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§ã®ãŸã‚
                failed_records=len(errors),
                errors=errors[:10] if errors else None  # æœ€åˆã®10ä»¶ã®ã¿
            )
            
        except Exception as e:
            db.rollback()
            print(f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise HTTPException(status_code=500, detail=f"WBGTå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def get_data_summary(
        self,
        db: Session,
        competition_id: Optional[str] = None
    ) -> DataSummaryResponse:
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼å–å¾—"""
        
        # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        query = db.query(RawSensorData)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        total_records = query.count()
        mapped_records = query.filter(RawSensorData.mapping_status == SensorDataStatus.MAPPED).count()
        unmapped_records = query.filter(RawSensorData.mapping_status == SensorDataStatus.UNMAPPED).count()
        
        # ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        sensor_counts = {}
        for sensor_type in SensorType:
            count = query.filter(RawSensorData.sensor_type == sensor_type).count()
            sensor_counts[sensor_type.value] = count
        
        wbgt_count = db.query(WBGTData)
        if competition_id:
            wbgt_count = wbgt_count.filter_by(competition_id=competition_id)
        wbgt_count = wbgt_count.count()
        
        mapping_count = db.query(FlexibleSensorMapping)
        if competition_id:
            mapping_count = mapping_count.filter_by(competition_id=competition_id)
        mapping_count = mapping_count.count()
        
        return DataSummaryResponse(
            total_sensor_records=total_records,
            mapped_records=mapped_records,
            unmapped_records=unmapped_records,
            sensor_type_counts=sensor_counts,
            wbgt_records=wbgt_count,
            mapping_records=mapping_count,
            competition_id=competition_id
        )

    def get_mapping_status(
        self,
        db: Session,
        competition_id: Optional[str] = None
    ) -> MappingStatusResponse:
        """ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³ç¢ºèª"""
        
        query = db.query(RawSensorData)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        status_counts = {}
        for status in SensorDataStatus:
            count = query.filter_by(mapping_status=status).count()
            status_counts[status.value] = count
        
        unmapped_by_type = {}
        for sensor_type in SensorType:
            count = query.filter(
                RawSensorData.sensor_type == sensor_type,
                RawSensorData.mapping_status == SensorDataStatus.UNMAPPED
            ).count()
            unmapped_by_type[sensor_type.value] = count
        
        return MappingStatusResponse(
            status_counts=status_counts,
            unmapped_by_sensor_type=unmapped_by_type,
            competition_id=competition_id
        )

    def get_unmapped_sensors(
        self,
        db: Session,
        sensor_type: Optional[SensorType] = None,
        competition_id: Optional[str] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """æœªãƒãƒƒãƒ—ã‚»ãƒ³ã‚µãƒ¼ä¸€è¦§å–å¾—"""
        
        query = db.query(RawSensorData).filter_by(mapping_status=SensorDataStatus.UNMAPPED)
        
        if sensor_type:
            query = query.filter_by(sensor_type=sensor_type)
        if competition_id:
            query = query.filter_by(competition_id=competition_id)
        
        # ã‚»ãƒ³ã‚µãƒ¼IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        unmapped_sensors = query.with_entities(
            RawSensorData.sensor_id,
            RawSensorData.sensor_type,
            RawSensorData.competition_id
        ).distinct().limit(limit).all()
        
        return [
            {
                'sensor_id': sensor.sensor_id,
                'sensor_type': sensor.sensor_type.value,
                'competition_id': sensor.competition_id
            }
            for sensor in unmapped_sensors
        ]

    def _detect_race_phases(self, record_data: dict) -> dict:
        """SWIM/BIKE/RUNåŒºé–“è‡ªå‹•åˆ¤å®šï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•èƒŒæ™¯è‰²ç”¨ï¼‰"""
        phases = {
            'swim_phase': None,
            'bike_phase': None, 
            'run_phase': None,
            'total_phase': None,
            'transition_phases': []  # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æœŸé–“
        }
        
        try:
            # å…¨ä½“ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»
            start_times = [record_data['swim_start'], record_data['bike_start'], record_data['run_start']]
            finish_times = [record_data['swim_finish'], record_data['bike_finish'], record_data['run_finish']]
            
            start_times = [t for t in start_times if t is not None]
            finish_times = [t for t in finish_times if t is not None]
            
            if start_times and finish_times:
                total_start = min(start_times)
                total_finish = max(finish_times)
                phases['total_phase'] = {
                    'start': total_start,
                    'finish': total_finish,
                    'duration_seconds': (total_finish - total_start).total_seconds()
                }
            
            # å„ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ã®èƒŒæ™¯è‰²ç”¨ï¼‰
            if record_data['swim_start'] and record_data['swim_finish']:
                swim_duration = (record_data['swim_finish'] - record_data['swim_start']).total_seconds()
                phases['swim_phase'] = {
                    'start': record_data['swim_start'],
                    'finish': record_data['swim_finish'],
                    'duration_seconds': swim_duration,
                    'phase_type': 'swim'
                }
            
            if record_data['bike_start'] and record_data['bike_finish']:
                bike_duration = (record_data['bike_finish'] - record_data['bike_start']).total_seconds()
                phases['bike_phase'] = {
                    'start': record_data['bike_start'],
                    'finish': record_data['bike_finish'],
                    'duration_seconds': bike_duration,
                    'phase_type': 'bike'
                }
            
            if record_data['run_start'] and record_data['run_finish']:
                run_duration = (record_data['run_finish'] - record_data['run_start']).total_seconds()
                phases['run_phase'] = {
                    'start': record_data['run_start'],
                    'finish': record_data['run_finish'],
                    'duration_seconds': run_duration,
                    'phase_type': 'run'
                }
            
            # ğŸ†• ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æœŸé–“ã®æ¤œå‡ºï¼ˆç«¶æŠ€é–“ã®ç§»è¡Œæ™‚é–“ï¼‰
            transitions = []
            
            # SWIM â†’ BIKE ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            if (record_data['swim_finish'] and record_data['bike_start'] and 
                record_data['bike_start'] > record_data['swim_finish']):
                t1_duration = (record_data['bike_start'] - record_data['swim_finish']).total_seconds()
                transitions.append({
                    'name': 'T1_transition',
                    'start': record_data['swim_finish'],
                    'finish': record_data['bike_start'],
                    'duration_seconds': t1_duration,
                    'phase_type': 'transition'
                })
            
            # BIKE â†’ RUN ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            if (record_data['bike_finish'] and record_data['run_start'] and 
                record_data['run_start'] > record_data['bike_finish']):
                t2_duration = (record_data['run_start'] - record_data['bike_finish']).total_seconds()
                transitions.append({
                    'name': 'T2_transition',
                    'start': record_data['bike_finish'],
                    'finish': record_data['run_start'],
                    'duration_seconds': t2_duration,
                    'phase_type': 'transition'
                })
            
            phases['transition_phases'] = transitions
            
            # ğŸ†• LAPæ™‚åˆ»ã®è§£æï¼ˆBL1, BL2...ã‹ã‚‰åŒºé–“æ¨å®šï¼‰
            if record_data['laps']:
                lap_analysis = self._analyze_lap_times(record_data['laps'], phases)
                phases['lap_analysis'] = lap_analysis
            
            # ğŸ†• ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ç”¨ã®æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            phases['graph_segments'] = self._generate_graph_segments(phases)
            
        except Exception as e:
            print(f"åŒºé–“åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚basestructureã¯è¿”ã™
            phases['error'] = str(e)
        
        return phases

    def _analyze_lap_times(self, laps: dict, phases: dict) -> dict:
        """LAPæ™‚åˆ»ã®è§£æï¼ˆBL1, BL2...ã‹ã‚‰ã®è©³ç´°åŒºé–“æ¨å®šï¼‰"""
        lap_analysis = {
            'total_laps': len(laps),
            'lap_times': [],
            'estimated_segments': []
        }
        
        try:
            # LAPæ™‚åˆ»ã‚’ã‚½ãƒ¼ãƒˆ
            sorted_laps = sorted(laps.items(), key=lambda x: x[1] if x[1] else datetime.min)
            
            for i, (lap_name, lap_time) in enumerate(sorted_laps):
                if lap_time is None:
                    continue
                    
                lap_info = {
                    'lap_name': lap_name,
                    'lap_time': lap_time,
                    'lap_number': i + 1
                }
                
                # å‰ã®LAPã¨ã®æ™‚é–“å·®è¨ˆç®—
                if i > 0:
                    prev_time = sorted_laps[i-1][1]
                    if prev_time:
                        interval_seconds = (lap_time - prev_time).total_seconds()
                        lap_info['interval_from_previous'] = interval_seconds
                
                # ç«¶æŠ€é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“
                if phases.get('total_phase') and phases['total_phase'].get('start'):
                    total_start = phases['total_phase']['start']
                    elapsed_seconds = (lap_time - total_start).total_seconds()
                    lap_info['elapsed_from_start'] = elapsed_seconds
                
                lap_analysis['lap_times'].append(lap_info)
            
            # ğŸ†• LAPæ™‚åˆ»ã‹ã‚‰ç«¶æŠ€åŒºé–“ã®æ¨å®š
            lap_analysis['estimated_segments'] = self._estimate_segments_from_laps(
                sorted_laps, phases
            )
            
        except Exception as e:
            lap_analysis['error'] = str(e)
            print(f"LAPè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return lap_analysis

    def _estimate_segments_from_laps(self, sorted_laps, phases: dict) -> list:
        """LAPæ™‚åˆ»ã‹ã‚‰ç«¶æŠ€åŒºé–“ã‚’æ¨å®šï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿BL/RLå¯¾å¿œï¼‰"""
        segments = []
        
        try:
            if not sorted_laps or len(sorted_laps) < 1:
                return segments
            
            # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼šBL(ãƒã‚¤ã‚¯LAP)ã¨RL(ãƒ©ãƒ³LAP)ã®åŒºåˆ¥
            bike_laps = [(name, time) for name, time in sorted_laps if name.upper().startswith('BL')]
            run_laps = [(name, time) for name, time in sorted_laps if name.upper().startswith('RL')]
            
            # ãƒã‚¤ã‚¯LAPåŒºé–“
            if bike_laps:
                bike_laps.sort(key=lambda x: x[1])  # æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆ
                segments.append({
                    'segment_type': 'bike_lap_segment',
                    'start_lap': bike_laps[0][0],
                    'end_lap': bike_laps[-1][0],
                    'start_time': bike_laps[0][1],
                    'end_time': bike_laps[-1][1],
                    'lap_count': len(bike_laps),
                    'confidence': 'high'  # BLåˆ—ãªã®ã§ç¢ºå®Ÿã«ãƒã‚¤ã‚¯åŒºé–“
                })
            
            # ãƒ©ãƒ³LAPåŒºé–“
            if run_laps:
                run_laps.sort(key=lambda x: x[1])  # æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆ
                segments.append({
                    'segment_type': 'run_lap_segment',
                    'start_lap': run_laps[0][0],
                    'end_lap': run_laps[-1][0],
                    'start_time': run_laps[0][1],
                    'end_time': run_laps[-1][1],
                    'lap_count': len(run_laps),
                    'confidence': 'high'  # RLåˆ—ãªã®ã§ç¢ºå®Ÿã«ãƒ©ãƒ³åŒºé–“
                })
            
            # ğŸ†• å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåŒºé–“æ¨å®šãƒ­ã‚¸ãƒƒã‚¯
            # START â†’ SFï¼šã‚¹ã‚¤ãƒ åŒºé–“
            # SF â†’ BSï¼šç¬¬1ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³
            # BS â†’ BL1ã€œBLnï¼šãƒã‚¤ã‚¯åŒºé–“
            # BLn â†’ RSï¼šç¬¬2ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ï¼ˆãƒã‚¤ã‚¯çµ‚äº†åˆ¤å®šãŒå¿…è¦ãªå ´åˆï¼‰
            # RS â†’ RL1ã€œRLn â†’ RFï¼šãƒ©ãƒ³åŒºé–“
            
            # ã‚ˆã‚Šè©³ç´°ãªè§£æï¼ˆå®Ÿæ¸¬å€¤ã¨çµ„ã¿åˆã‚ã›ï¼‰
            all_times = [(name, time) for name, time in sorted_laps if time]
            all_times.sort(key=lambda x: x[1])
            
            if all_times:
                segments.append({
                    'segment_type': 'total_lap_coverage',
                    'start_lap': all_times[0][0],
                    'end_lap': all_times[-1][0],
                    'start_time': all_times[0][1],
                    'end_time': all_times[-1][1],
                    'total_laps': len(all_times),
                    'bike_laps': len(bike_laps),
                    'run_laps': len(run_laps)
                })
            
        except Exception as e:
            print(f"å®Ÿãƒ‡ãƒ¼ã‚¿åŒºé–“æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return segments

    def _generate_graph_segments(self, phases: dict) -> list:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ•ç”¨ã®æ™‚é–“è»¸ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        segments = []
        
        try:
            # ç¢ºå®šåŒºé–“ï¼ˆå®Ÿéš›ã®START/FINISHæ™‚åˆ»ã‹ã‚‰ï¼‰
            for phase_name in ['swim_phase', 'bike_phase', 'run_phase']:
                phase = phases.get(phase_name)
                if phase and phase.get('start') and phase.get('finish'):
                    segments.append({
                        'segment_type': phase['phase_type'],
                        'start_time': phase['start'],
                        'end_time': phase['finish'],
                        'duration_seconds': phase['duration_seconds'],
                        'confidence': 'high',  # å®Ÿæ¸¬å€¤ãªã®ã§é«˜ä¿¡é ¼åº¦
                        'background_color': self._get_phase_color(phase['phase_type'])
                    })
            
            # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³åŒºé–“
            for transition in phases.get('transition_phases', []):
                segments.append({
                    'segment_type': 'transition',
                    'start_time': transition['start'],
                    'end_time': transition['finish'],
                    'duration_seconds': transition['duration_seconds'],
                    'confidence': 'high',
                    'background_color': '#f0f0f0'  # ã‚°ãƒ¬ãƒ¼
                })
            
            # LAPæ¨å®šåŒºé–“ï¼ˆå®Ÿæ¸¬å€¤ãŒãªã„å ´åˆã®è£œå®Œï¼‰
            lap_analysis = phases.get('lap_analysis', {})
            if lap_analysis.get('estimated_segments') and not segments:
                # å®Ÿæ¸¬å€¤ãŒãªã„å ´åˆã®ã¿LAPæ¨å®šã‚’ä½¿ç”¨
                for est_segment in lap_analysis['estimated_segments']:
                    segments.append({
                        'segment_type': est_segment['segment_type'],
                        'start_time': est_segment['start_time'],
                        'end_time': est_segment['end_time'],
                        'confidence': 'medium',  # æ¨å®šå€¤ãªã®ã§ä¸­ä¿¡é ¼åº¦
                        'background_color': self._get_phase_color(
                            est_segment['segment_type'].replace('estimated_', '')
                        )
                    })
            
            # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
            segments.sort(key=lambda x: x['start_time'])
            
        except Exception as e:
            print(f"ã‚°ãƒ©ãƒ•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return segments

    def _get_phase_color(self, phase_type: str) -> str:
        """ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚ºã®èƒŒæ™¯è‰²ã‚’å–å¾—"""
        colors = {
            'swim': '#e3f2fd',    # æ°´è‰²ï¼ˆSWIMï¼‰
            'bike': '#fff3e0',    # ã‚ªãƒ¬ãƒ³ã‚¸ï¼ˆBIKEï¼‰
            'run': '#e8f5e8',     # ç·‘ï¼ˆRUNï¼‰
            'transition': '#f5f5f5'  # ã‚°ãƒ¬ãƒ¼ï¼ˆãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ï¼‰
        }
        return colors.get(phase_type, '#ffffff')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç™½# app/services/flexible_csv_service.py ã«è¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰

    # app/services/flexible_csv_service.py - ç°¡ç´ ç‰ˆï¼ˆä¸Šæ›¸ãæ©Ÿèƒ½å‰Šé™¤ï¼‰

    async def process_race_record_data(
        self,
        race_files: List[UploadFile],
        competition_id: str,
        db: Session
    ) -> dict:
        """
        å¤§ä¼šè¨˜éŒ²ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆç°¡ç´ ç‰ˆï¼‰
        - ä¸Šæ›¸ãæ©Ÿèƒ½å‰Šé™¤
        - ã‚·ãƒ³ãƒ—ãƒ«ãªè¿½åŠ ã®ã¿
        - ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤å¯èƒ½
        """
        try:
            # å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from app.models.competition import Competition, RaceRecord
            from app.models.user import AdminUser
            from app.models.flexible_sensor_data import UploadBatch, SensorType, UploadStatus
            
            competition = db.query(Competition).filter_by(competition_id=competition_id).first()
            if not competition:
                raise HTTPException(status_code=400, detail=f"å¤§ä¼šID '{competition_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ãƒãƒƒãƒIDç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_id = f"{timestamp}_race_records_{len(race_files)}files"
            
            # å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ
            all_records = {}  # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ã‚’ã‚­ãƒ¼ã¨ã—ãŸçµ±åˆãƒ‡ãƒ¼ã‚¿
            lap_columns = set()  # æ¤œå‡ºã•ã‚ŒãŸLAPåˆ—å
            total_files_processed = 0
            total_csv_records = 0
            errors = []
            
            for file_idx, file in enumerate(race_files):
                try:
                    print(f"Processing file {file_idx + 1}/{len(race_files)}: {file.filename}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    content = await file.read()
                    if not content:
                        errors.append(f"{file.filename}: ç©ºãƒ•ã‚¡ã‚¤ãƒ«")
                        continue
                    
                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
                    detected_encoding = None
                    for encoding in ['utf-8', 'shift_jis', 'cp932', 'iso-8859-1']:
                        try:
                            decoded_content = content.decode(encoding)
                            detected_encoding = encoding
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if detected_encoding is None:
                        errors.append(f"{file.filename}: æ–‡å­—ã‚³ãƒ¼ãƒ‰èªè­˜å¤±æ•—")
                        continue
                    
                    print(f"  ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
                    
                    # CSVã‚’DataFrameã«èª­ã¿è¾¼ã¿
                    df = pd.read_csv(io.StringIO(decoded_content))
                    
                    if df.empty:
                        errors.append(f"{file.filename}: ãƒ‡ãƒ¼ã‚¿ãŒç©º")
                        continue
                    
                    total_csv_records += len(df)
                    
                    # åˆ—ã®è©³ç´°åˆ†æ
                    print(f"  CSVåˆ—: {list(df.columns)}")
                    print(f"  ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(df)}")
                    
                    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼šã‚¼ãƒƒã‚±ãƒ³ç•ªå·ï¼ˆ"No."åˆ—ï¼‰
                    bib_number_col = None
                    for col in df.columns:
                        col_clean = str(col).strip().lower()
                        if col_clean in ['no.', 'no', 'bib', 'number', 'ã‚¼ãƒƒã‚±ãƒ³', 'ãƒŠãƒ³ãƒãƒ¼']:
                            bib_number_col = col
                            break
                    
                    if bib_number_col is None:
                        errors.append(f"{file.filename}: ã‚¼ãƒƒã‚±ãƒ³ç•ªå·åˆ—ï¼ˆNo.ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        continue
                    
                    print(f"  ã‚¼ãƒƒã‚±ãƒ³ç•ªå·åˆ—: '{bib_number_col}'")
                    
                    # æ™‚åˆ»é–¢é€£åˆ—ã®æ¤œå‡º
                    time_columns = []
                    for col in df.columns:
                        col_clean = str(col).strip().upper()
                        if any(keyword in col_clean for keyword in [
                            'START', 'FINISH', 'SF', 'BS', 'RS', 'RF',
                            'BL1', 'BL2', 'BL3', 'BL4', 'BL5',  # ãƒã‚¤ã‚¯LAP
                            'RL1', 'RL2', 'RL3', 'RL4', 'RL5',  # ãƒ©ãƒ³LAP
                            'TIME', 'æ™‚åˆ»', 'LAP'
                        ]):
                            time_columns.append(col)
                            if col_clean.startswith(('BL', 'RL')):
                                lap_columns.add(col)
                    
                    print(f"  æ¤œå‡ºã•ã‚ŒãŸæ™‚åˆ»åˆ—: {time_columns}")
                    if lap_columns:
                        print(f"  æ¤œå‡ºã•ã‚ŒãŸLAPåˆ—: {sorted(lap_columns)}")
                    
                    # å„è¡Œã‚’å‡¦ç†
                    file_records = 0
                    for index, row in df.iterrows():
                        try:
                            # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·å–å¾—
                            bib_number_raw = row[bib_number_col]
                            if pd.isna(bib_number_raw):
                                continue
                            
                            bib_number = str(bib_number_raw).strip()
                            if not bib_number or bib_number.lower() == 'nan':
                                continue
                            
                            # çµ±åˆãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ/æ›´æ–°
                            if bib_number not in all_records:
                                all_records[bib_number] = {
                                    'race_number': bib_number,
                                    'competition_id': competition_id,
                                    'swim_start_time': None,
                                    'swim_finish_time': None,
                                    'bike_start_time': None,
                                    'bike_finish_time': None,
                                    'run_start_time': None,
                                    'run_finish_time': None,
                                    'times': {},
                                    'metadata': {}
                                }
                            
                            record = all_records[bib_number]
                            
                            # æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                            for time_col in time_columns:
                                time_value = row.get(time_col)
                                if pd.notna(time_value):
                                    time_str = str(time_value).strip()
                                    
                                    if self._is_likely_time_data(time_str):
                                        parsed_time = self._parse_race_time(time_value)
                                        if parsed_time:
                                            record['times'][time_col] = parsed_time
                                            
                                            # ä¸»è¦æ™‚åˆ»ã®è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°
                                            col_upper = time_col.upper()
                                            if col_upper == 'START':
                                                record['swim_start_time'] = parsed_time
                                            elif col_upper == 'SF':  # Swim Finish
                                                record['swim_finish_time'] = parsed_time
                                            elif col_upper == 'BS':  # Bike Start
                                                record['bike_start_time'] = parsed_time
                                            elif col_upper == 'RS':  # Run Start
                                                record['run_start_time'] = parsed_time
                                            elif col_upper == 'RF':  # Run Finish
                                                record['run_finish_time'] = parsed_time
                                    else:
                                        # æ™‚åˆ»ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                                        record['metadata'][time_col] = time_str
                            
                            # åŸºæœ¬æƒ…å ±ã®åé›†ï¼ˆæ™‚åˆ»åˆ—ä»¥å¤–ï¼‰
                            for col in df.columns:
                                if col not in time_columns and col != bib_number_col:
                                    value = row.get(col)
                                    if pd.notna(value):
                                        record['metadata'][col] = str(value).strip()
                            
                            file_records += 1
                            
                        except Exception as e:
                            print(f"  è¡Œ{index+1}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    print(f"  âœ… {file.filename}: {file_records}ä»¶ã®è¨˜éŒ²ã‚’å‡¦ç†")
                    total_files_processed += 1
                    
                except Exception as e:
                    errors.append(f"{file.filename}: {str(e)}")
                    print(f"  âŒ {file.filename}: {e}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            saved_count = 0
            failed_count = 0
            
            for race_number, record in all_records.items():
                try:
                    race_record = RaceRecord(
                        competition_id=competition_id,
                        race_number=race_number,
                        user_id=None,  # ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã«è¨­å®š
                        swim_start_time=record['swim_start_time'],
                        swim_finish_time=record['swim_finish_time'],
                        bike_start_time=record['bike_start_time'],
                        bike_finish_time=record['bike_finish_time'],
                        run_start_time=record['run_start_time'],
                        run_finish_time=record['run_finish_time'],
                        notes=f"æ¤œå‡ºæ™‚åˆ»æ•°: {len(record['times'])}"
                    )
                    
                    # LAP ãƒ‡ãƒ¼ã‚¿è¨­å®šï¼ˆJSONã¨ã—ã¦ä¿å­˜ï¼‰
                    if record['times']:
                        import json
                        times_str = {}
                        for key, dt in record['times'].items():
                            if isinstance(dt, datetime):
                                times_str[key] = dt.isoformat()
                            else:
                                times_str[key] = str(dt)
                        race_record.lap_data = json.dumps(times_str)
                    
                    db.add(race_record)
                    saved_count += 1
                    
                except Exception as e:
                    failed_count += 1
                    print(f"DBä¿å­˜ã‚¨ãƒ©ãƒ¼ (ã‚¼ãƒƒã‚±ãƒ³{race_number}): {e}")
            
            # ãƒãƒƒãƒè¨˜éŒ²ä½œæˆ
            admin_user = db.query(AdminUser).first()
            admin_id = admin_user.admin_id if admin_user else "system"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç·ã‚µã‚¤ã‚ºè¨ˆç®—
            total_file_size = 0
            for file in race_files:
                await file.seek(0)
                content = await file.read()
                total_file_size += len(content)
                await file.seek(0)
            
            batch = UploadBatch(
                batch_id=batch_id,
                sensor_type=SensorType.OTHER,
                competition_id=competition_id,
                file_name=f"race_records_{len(race_files)}files.csv",
                file_size=total_file_size,
                total_records=total_csv_records,
                success_records=saved_count,
                failed_records=failed_count,
                status=UploadStatus.SUCCESS if failed_count == 0 else UploadStatus.PARTIAL,
                uploaded_by=admin_id,
                notes=f"çµ±åˆLAPåˆ—: {', '.join(sorted(lap_columns))}" if lap_columns else None
            )
            db.add(batch)
            
            db.commit()
            
            # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            message = f"å¤§ä¼šè¨˜éŒ²ã‚’{saved_count}ä»¶å‡¦ç†ã—ã¾ã—ãŸ"
            if failed_count > 0:
                message += f"ï¼ˆå¤±æ•—: {failed_count}ä»¶ï¼‰"
            
            return {
                "success": saved_count > 0,
                "message": message,
                "total_files": len(race_files),
                "processed_files": total_files_processed,
                "total_csv_records": total_csv_records,
                "saved_records": saved_count,
                "failed_records": failed_count,
                "detected_lap_columns": sorted(lap_columns),
                "batch_id": batch_id,
                "errors": errors[:10] if errors else []
            }
            
        except HTTPException:
            raise
        except Exception as e:
            db.rollback()
            error_message = f"å¤§ä¼šè¨˜éŒ²å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(error_message)
            raise HTTPException(status_code=500, detail=error_message)

    def _is_likely_time_data(self, time_str: str) -> bool:
        """æ–‡å­—åˆ—ãŒæ™‚åˆ»ãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ãŒé«˜ã„ã‹ãƒã‚§ãƒƒã‚¯"""
        if not time_str or len(time_str.strip()) == 0:
            return False
        
        time_str = time_str.strip()
        
        # æ˜ã‚‰ã‹ã«æ™‚åˆ»ã§ã¯ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
        non_time_patterns = [
            'é¸æ‰‹_', 'éƒ¨é–€', 'ç”·æ€§', 'å¥³æ€§', 'çœŒ', 'å®Œèµ°', 'DNS', 'DNF', 'DQ',
            'LONG', 'SHORT', 'ã‚¹ãƒ—ãƒªãƒ³ãƒˆ', 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯'
        ]
        
        for pattern in non_time_patterns:
            if pattern in time_str:
                return False
        
        # æ•°å­—ã®ã¿ã§æ§‹æˆã•ã‚Œã€ã‹ã¤1-4æ¡ã®å ´åˆã¯æ™‚åˆ»ã®å¯èƒ½æ€§ãŒä½ã„
        # ï¼ˆãŸã ã—ã€ç§’æ•°ã‚„åˆ†æ•°ã®å ´åˆã‚‚ã‚ã‚‹ã®ã§å®Œå…¨é™¤å¤–ã¯ã—ãªã„ï¼‰
        if time_str.isdigit():
            num = int(time_str)
            # ã‚¼ãƒƒã‚±ãƒ³ç•ªå·ã‚„å¹´é½¢ã®å¯èƒ½æ€§ãŒé«˜ã„ç¯„å›²
            if 1 <= num <= 9999:
                # ã•ã‚‰ã«è©³ç´°ãƒã‚§ãƒƒã‚¯ï¼šæ™‚åˆ»ã¨ã—ã¦å¦¥å½“ã‹ã©ã†ã‹
                if num <= 23:  # æ™‚é–“ã¨ã—ã¦å¦¥å½“
                    return True
                elif num <= 59:  # åˆ†ãƒ»ç§’ã¨ã—ã¦å¦¥å½“
                    return True
                else:
                    return False
        
        # æ™‚åˆ»ã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_patterns = [
            ':',  # HH:MM:SSå½¢å¼
            '-',  # æ—¥ä»˜å½¢å¼
            '/',  # æ—¥ä»˜å½¢å¼
            '.'   # å°æ•°ç‚¹ï¼ˆç§’ã®å°æ•°éƒ¨ï¼‰
        ]
        
        return any(pattern in time_str for pattern in time_patterns)

    def _parse_race_time(self, time_value) -> Optional[datetime]:
        """ãƒ¬ãƒ¼ã‚¹æ™‚åˆ»ã®æŸ”è»Ÿãªè§£æï¼ˆã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›æŠ‘åˆ¶ç‰ˆï¼‰"""
        if pd.isna(time_value):
            return None
        
        time_str = str(time_value).strip()
        if not time_str or time_str.lower() in ['nan', '', 'null']:
            return None
        
        try:
            # è¤‡æ•°ã®æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M:%S.%f",
                "%Y/%m/%d %H:%M:%S",
                "%d/%m/%Y %H:%M:%S",
                "%H:%M:%S",
                "%H:%M:%S.%f",
                "%M:%S",  # åˆ†:ç§’
                "%S.%f"   # ç§’.ãƒŸãƒªç§’
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(time_str, fmt)
                except ValueError:
                    continue
            
            # pandas to_datetimeã§æœ€å¾Œã®è©¦è¡Œ
            return pd.to_datetime(time_str, errors='coerce')
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ãªã„ï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿ï¼‰
            # print(f"æ™‚åˆ»è§£æã‚¨ãƒ©ãƒ¼: {time_str} - {e}")
            return None

    def _detect_race_phases(self, record_data: dict) -> dict:
        """SWIM/BIKE/RUNåŒºé–“è‡ªå‹•åˆ¤å®š"""
        phases = {
            'swim_phase': None,
            'bike_phase': None, 
            'run_phase': None,
            'total_phase': None
        }
        
        try:
            # å…¨ä½“ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»
            start_times = [record_data['swim_start'], record_data['bike_start'], record_data['run_start']]
            finish_times = [record_data['swim_finish'], record_data['bike_finish'], record_data['run_finish']]
            
            start_times = [t for t in start_times if t is not None]
            finish_times = [t for t in finish_times if t is not None]
            
            if start_times and finish_times:
                total_start = min(start_times)
                total_finish = max(finish_times)
                phases['total_phase'] = {'start': total_start, 'finish': total_finish}
            
            # å„ç«¶æŠ€ãƒ•ã‚§ãƒ¼ã‚º
            if record_data['swim_start'] and record_data['swim_finish']:
                phases['swim_phase'] = {
                    'start': record_data['swim_start'],
                    'finish': record_data['swim_finish']
                }
            
            if record_data['bike_start'] and record_data['bike_finish']:
                phases['bike_phase'] = {
                    'start': record_data['bike_start'],
                    'finish': record_data['bike_finish']
                }
            
            if record_data['run_start'] and record_data['run_finish']:
                phases['run_phase'] = {
                    'start': record_data['run_start'],
                    'finish': record_data['run_finish']
                }
            
        except Exception as e:
            print(f"åŒºé–“åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return phases